* cfDNA Copy Numer Alteration Analysis :biopipe:
:PROPERTIES:
:logging: nil
:header-args:bash: :tangle-mode (identity #0555)
:END:
** README
*** Change Log
- [2022-04-29 Fri]: First commit, copying from the old mpnst-cna repo. Untested. 
** Setup
*** Snakemake
**** Configuration YAMLs
- repo_test
  #+begin_src bash :tangle ./config/repo_test.yaml
container: "/home/jeszyman/sing_containers/mpnst.sif"
log_dir: "test/logs"
threads: "4"
inputs_dir: "test/inputs"
bam_dir: "test/bam"
frag_bam_dir: "test/frag-bam"
wig_dir: "test/wig"
ichor_dir: "test/ichor"
#+end_src
**** Run commands
#+begin_src bash :tangle ./src/smk_repo_test.sh
eval "$(command conda 'shell.bash' 'hook' 2> /dev/null)"

conda activate snakemake

output_dirs=( "frag-bam" )

for dir in ${output_dirs[@]};
do
               if [ -d test/${dir} ]; then \rm -rf test/${dir}; fi
done

snakemake \
    --configfile config/repo_test.yaml \
    --cores 4 \
    --rerun-incomplete \
    --use-singularity \
    --forceall \
    --snakefile ./workflow/cna.smk 
#+end_src
#+begin_src bash :tangle ./src/smk_draw.sh
eval "$(command conda 'shell.bash' 'hook' 2> /dev/null)"

conda activate snakemake

snakemake \
    --configfile config/repo_test.yaml \
    --cores $threads \
    --rulegraph \
    --snakefile ./workflow/read_preprocess.smk | dot -Tpdf > resources/read_preprocess_dagtmp/test.pdf
#+end_src

*** Repository
#+begin_src bash
mkdir -p config
mkdir -p resources
mkdir -p source
mkdir -p test
mkdir -p workflow/scripts
#+end_src
*** Emacs and Org-mode
#+startup: shrink
*** Integration testing setup
#+begin_src bash
mkdir -p test/bam
mkdir -p test/inputs

# Samples file manually created at test/inputs/samples/tsv

singularity shell --bind /mnt:/mnt ~/sing_containers/biotools.sif 

function SubSample {

## Calculate the sampling factor based on the intended number of reads:
FACTOR=$(samtools idxstats $1 | cut -f3 | awk -v COUNT=$2 'BEGIN {total=0} {total += $1} END {print COUNT/total}')

if [[ $FACTOR > 1 ]]
  then 
  echo '[ERROR]: Requested number of reads exceeds total read count in' $1 '-- exiting' && exit 1
fi

sambamba view -s $FACTOR -f bam -l 5 $1

}

SubSample /mnt/ris/aadel/mpnst/bam/lib081_dedup_sorted.bam 100000 > test/bam/lib001.bam
SubSample /mnt/ris/aadel/mpnst/bam/lib082_dedup_sorted.bam 100000 > test/bam/lib002.bam
#+end_src
** TODO Repo-local integration testing                                  :smk:
#+begin_src snakemake :tangle ./workflow/cfdna_cna_int_test.smk
container: config["container"]

IDS, = glob_wildcards(config["cna_bam_dir"] + "/{id}.bam"

rule all:
    input:
        expand(config["wig_dir"] + "/{library_id}_frag{frag_distro}.wig", library_id = IDS, frag_distro = ["90_150"]),
        expand(config["ichor_dir"] + "/{library_id}_frag{frag_distro}.file", library_id = IDS, frag_distro = ["90_150"]),
 
#+end_src
** cfDNA WGS CNA :smk:
:PROPERTIES:
:header-args:snakemake: :tangle ./workflow/cfdna_wgs_cna.smk
:END:
*** Fragment size filtering
#+begin_src snakemake
rule frag_filt:
    input:
        config["bam_dir"] + "/{library_id}.bam"
    params:
        out_dir = config["frag_bam_dir"]
    output:
        nohead = temp(config["frag_bam_dir"] + "/{library_id}_frag{frag_distro}.nohead"),
        onlyhead = temp(config["frag_bam_dir"] + "/{library_id}_frag{frag_distro}.onlyhead"),
        final = config["frag_bam_dir"] + "/{library_id}_frag{frag_distro}.bam",
    shell:
        """
        frag_min=$(echo {wildcards.frag_distro} | sed -e "s/_.*$//g")
        frag_max=$(echo {wildcards.frag_distro} | sed -e "s/^.*_//g")
        workflow/scripts/frag_filt.sh {input} \
                                      {output.nohead} \
                                      $frag_min \
                                      $frag_max \
                                      {config[threads]} \
                                      {output.onlyhead} \
                                      {output.final}
        """
#+end_src

#+begin_src bash :tangle ./workflow/frag_filt.sh
#test            
#+end_src

#+begin_src bash :tangle ./workflow/scripts/frag_filt.sh

# Steps
## Filter by absolute value of TLEN for each read
sambamba view -t $5 $1 | awk -F'\t' -v upper="$4" 'sqrt($9*$9) < upper {print $0}' | awk -F'\t' -v lower="$3" 'sqrt($9*$9) > lower {print $0}'> $2

## Restore header
sambamba view -H $1 > $6

cat $6 $2 | sambamba view -t 4 -S -f bam /dev/stdin | sambamba sort -t 4 -o $7 /dev/stdin 

#+end_src        
*** Convert bam to wig
#+begin_src snakemake
rule bam_to_wig:
    input: config["frag_bam_dir"] + "/{library_id}_frag{frag_distro}.bam",
    output: config["wig_dir"] + "/{library_id}_frag{frag_distro}.wig",
    shell:
        """
        /opt/hmmcopy_utils/bin/readCounter --window 1000000 --quality 20 \
        --chromosome "chr1,chr2,chr3,chr4,chr5,chr6,chr7,chr8,chr9,chr10,chr11,chr12,chr13,chr14,chr15,chr16,chr17,chr18,chr19,chr20,chr21,chr22,chrX,chrY" \
        {input} > {output}
        """
#+end_src
*** Run ichor
#+begin_src snakemake
rule ichor:
    input:
        config["wig_dir"] + "/{library_id}_frag{frag_distro}.wig",
    output:
        config["ichor_dir"] + "/{library_id}_frag{frag_distro}.file",
    shell:
        """
        Rscript /opt/ichorCNA/scripts/runIchorCNA.R \
         --id {wildcards.library_id}_{wildcards.frag_distro} \
         --WIG {input} \
         --gcWig /opt/ichorCNA/inst/extdata/gc_hg19_1000kb.wig \
         --normal "c(0.95, 0.99, 0.995, 0.999)" \
         --ploidy "c(2)" \
         --maxCN 3 \
         --estimateScPrevalence FALSE \
         --scStates "c()" \
         --outDir {config[ichor_dir]}
        """
#+end_src

#+name: ichor_lowfract
#+begin_src bash :tangle ./src/functions.sh
ichor_lowfract() {
# Runs ichorCNA to generate tumor fraction
#  See https://doi.org/10.1038/s41467-017-00965-y
#
# Input parameters
#  $1 = input wig
#  $2 = output directory
#
# Steps
##
## Setup in-function parameters    
base=$(basename -s .wig $1)
##
## Check for inputs and outputs
if [ ! -f $1 ]; then
   echo "No input wig found"
elif [ $2/${base}.RData -nt $1 ]; then
   echo "wig for ${base} already processed in ichor"
else
   Rscript /opt/ichorCNA/scripts/runIchorCNA.R \
           --id $base \
           --WIG $1 \
           --gcWig /opt/ichorCNA/inst/extdata/gc_hg19_1000kb.wig \
           --normal "c(0.95, 0.99, 0.995, 0.999)" \
           --ploidy "c(2)" \
           --maxCN 3 \
           --estimateScPrevalence FALSE \
           --scStates "c()" \
           --outDir $2
fi
}
#
#+end_src

*** Dev
:PROPERTIES:
:header-args:snakemake: :tangle no
:END:
**** Aggregate ichor
**** CN LOH
https://github.com/mskcc/facets
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5027494/
- a CN LOH call is NOT available in ichor, is in titanCNA
- FACETS is used for CN-LOH in cfDNA- https://aacrjournals.org/clincancerres/article/28/3/526/678032/Activation-of-PI3K-AKT-Pathway-Is-a-Potential

  https://sites.google.com/site/mskfacets/
;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6267593/

facets for independent ichor confirm? https://github.com/mskcc/facets/issues/72
ichor does cn loh calls- check out


** Ideas
- ichor PON
- extract tf
  tfRAW = as_tibble(read.table(file.path(repo,"data/tf_summary"), header = F, sep = '\t'))

target_cnaRAW = as_tibble(read.table(file.path(repo,"data/target_cna.bed"), sep = '\t', header = F))

taylor_washoutRAW = as_tibble(read.csv(file.path(repo, "data/cfDNA PN and MPNST washout libraries for ROC.csv"), header = T))

coverageRAW = as_tibble(read.table(file.path(repo,"data/all_dedup_coverage.tsv"), sep='\t', header = T))

librariesRAW = as_tibble(
  read.csv(file.path(repo,"data/library_index.csv"), header = T)
)

washout_libs = as_tibble(
read.csv(file.path(repo,"data/washout_libs.csv"), header = T)
)

specimensRAW = as_tibble(
  read.csv(file.path(repo,"data/specimen_index.csv"), header = T)
  )

subjectsRAW = as_tibble(
  read.csv(file.path(repo, "data/subject_index.csv"), header = T)
  )

  #+begin_src R
library(tidyverse)

load("/mnt/ris/aadel/mpnst/data_model/data_model.RData")

ls()

names(libraries_full)

class(libraries_full$collect_date)

libraries_full$collect_date = as.Date(libraries_full$collect_date)

as.numeric(libraries_full$collect_date[[1]]- libraries_full$collect_date[[2]])

test =
  libraries_full %>% arrange(collect_date) %>% group_by(participant_id, isolation_type) %>%
  mutate(collect_day = as.numeric(collect_date - first(collect_date))) %>%
  mutate(collect_day = replace_na(collect_day, 0))

tf = read.table("/tmp/tf.tsv", header = F, sep = '\t')
colnames(tf) = c("libnfrag", "tf", "ploidy")
tf$library_id = substr(tf$libnfrag, 1, 6)

tf2 =
  tf %>% filter(grepl("sub20m_frag90", libnfrag))


test2=tf2 %>% left_join(test, by = "library_id")

write.csv(file ="/tmp/test.csv", test2)
test %>% select(participant_id, collect_day) %>% arrange(participant_id) %>% print(n = Inf)



test$collect_day

  case_when(collect_date == first(collect_date) ~ 0,
                                 collect_date > first(collect_date) ~ collect_date - first(collect_date)))




) %>% select(library_id, participant_id, collect_day)
#+end_src







